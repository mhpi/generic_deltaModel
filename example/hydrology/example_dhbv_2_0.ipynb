{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: **δHBV 2.0**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates forward simulation with a pre-trained δHBV 2.0UH model developed by [Yalan Song et al. (2025)](https://doi.org/10.1029/2024WR038928). For explanation of model structure, methodologies, [data](https://mhpi.github.io/datasets/CONUS/#results), and performance metrics, please refer to Song's publication [below](#publication). If you find this code is useful in your own work, please include the aforementioned citation.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Before Running:\n",
    "- **Environment**: See [setup.md](./../../docs/setup.md) for ENV setup. dMG must be installed with dependencies + hydroDL2 to run this notebook.\n",
    "\n",
    "- **Model and Data**: The pretrained δHBV 2.0 model weights + input data can be downloaded from [sharepoint](https://pennstateoffice365-my.sharepoint.com/:f:/g/personal/cxs1024_psu_edu/Eqi1NuJ3d2pMpEJpVu0EGSoBigi-VCWVHgOYIRoTeuGiOw?e=HaNNeA). After downloading, update model and data key paths in their respective configs:\n",
    "\n",
    "    1. In [`./generic_deltamodel/example/conf/config_dhbv_2_0.yaml`](./../conf/config_dhbv_2_0.yaml), update *trained_model* with your path to the parent directory containing both trained model weights `dHBV_2_0_Ep100.pt` **and** normalization file `normalization_statistics.json`.\n",
    "\n",
    "    2. In [`./generic_deltamodel/example/conf/observations/merit.yaml`](./../conf/observations/merit.yaml), update *subbasin_data_path* with your path to `./merit_71_0`.\n",
    "\n",
    "- **Hardware**: The NNs used in this model require CUDA support only available with Nvidia GPUs. For those without access, T4 GPUs can be used when running this notebook with dMG on [Google Colab](https://colab.research.google.com/).\n",
    "\n",
    "### Publication:\n",
    "*Yalan Song, Tadd Bindas, Chaopeng Shen, Haoyu Ji, Wouter Johannes Maria Knoben, Leo Lonzarich, Martyn P. Clark, et al. \"High-resolution national-scale water modeling is enhanced by multiscale differentiable physics-informed machine learning.\" Water Resources Research (2025). https://doi.org/10.1029/2024WR038928.*\n",
    "\n",
    "<br>\n",
    "\n",
    "### Issues:\n",
    "For questions, concerns, bugs, etc., please reach out by posting an [issue](https://github.com/mhpi/generic_deltaModel/issues) on GitHub.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Train/Evaluate $\\delta$ HBV 2.0\n",
    "\n",
    "*Multiscale training for dHBV2.0 is not currently enabled in dMG. Training code will be released at a later time.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward $\\delta$ HBV 2.0\n",
    "\n",
    "After completing [these](#before-running) steps, forward the $\\delta$ HBV 2.0 model with the code block below.\n",
    "\n",
    "--> For default settings expect evaluation time of ~1 minute with an Nvidia A100.\n",
    "\n",
    "**Note**\n",
    "- The settings defined in the config file `../example/conf/config_dhbv_2_0.yaml` are set to replecate benchmark performance.\n",
    "- For model evaluation, set `mode: predict` in the config, or modify after the config dict has been created (see below).\n",
    "- The default inference window is set from 1 January 1980 to 31 December 2020, which should use ~70GB of vram.\n",
    "- The first year (`warm_up` in the config, 365 days is default) of the inference period is used for initializing HBV's internal states (water storages) and is, therefore, excluded from the model's prediction output.\n",
    "- If you are new to the *dMG* framework and want further explanation and exposure of the methods used below, we suggest first looking at our notebook for $\\delta$ HBV 1.0: `example/hydrology/example_dhbv_1_0.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dMG'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdMG\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dMG'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import dMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../dMG')  # Add the dMG root directory.\n",
    "\n",
    "from dMG.utils import print_config\n",
    "from core.utils.factory import import_data_loader, import_trainer\n",
    "from example import load_config\n",
    "from models.model_handler import ModelHandler as dHBV\n",
    "\n",
    "#------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '../example/conf/config_dhbv_2_0.yaml'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "print_config(config)\n",
    "\n",
    "# 2. Setup a dataset loader to prepare NN and physics model inputs.\n",
    "data_loader_cls = import_data_loader(config['data_loader'])\n",
    "data_loader = data_loader_cls(config, test_split=True, overwrite=False)\n",
    "\n",
    "# 3. Initialize the differentiable model dHBV 2.0 (LSTM + HBV 2.0).\n",
    "model = dHBV(config, verbose=True)\n",
    "\n",
    "# 4. Initialize trainer to handle forward pass.\n",
    "trainer_cls = import_trainer(config['trainer'])\n",
    "trainer = trainer_cls(\n",
    "    config,\n",
    "    model,\n",
    "    inf_dataset=data_loader.dataset,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 5. Forward pass through the model to get streamflow predictions.\n",
    "predictions = trainer.inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Model Predictions\n",
    "\n",
    "After running model inference we can, e.g., view the hydrograph for one of the catchments to see we are getting expected outputs.\n",
    "\n",
    "We can do this with our target variable, streamflow, for instance... (though, there are many other states and fluxes we can output as shown in the output cell below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "from core.post.plot_hydrograph import plot_hydrograph\n",
    "from core.utils.dates import Dates\n",
    "\n",
    "#------------------------------------------#\n",
    "# Choose a catchment by unit catchment ID (COMID) to plot.\n",
    "COMID = 71024425\n",
    "TARGET = 'flow_sim'\n",
    "\n",
    "# Resample to 3-day prediction. Options: 'D', 'W', 'M', 'Y'.\n",
    "RESAMPLE = 'D'\n",
    "\n",
    "# Set the path to the zarr store of input data (containing COMIDs).\n",
    "DATA_PATH = 'your/path/to/MERIT_input_sample/71_0'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "print(f\"HBV states and fluxes: {predictions.keys()} \\n\")\n",
    "\n",
    "\n",
    "# 1. Get the streamflow predictions and daily timesteps of the prediction window.\n",
    "pred = predictions[TARGET]\n",
    "timesteps = Dates(config['predict'], config['dpl_model']['rho']).batch_daily_time_range\n",
    "\n",
    "# Remove warm-up period to match model output (see Note above.)\n",
    "timesteps = timesteps[config['dpl_model']['phy_model']['warm_up']:]\n",
    "\n",
    "\n",
    "# 2. Load array of comids and get the index of the selected catchment.\n",
    "root = zarr.open_group(DATA_PATH, mode='r+')\n",
    "comids = root['COMID'][:]\n",
    "print(f\"First 20 available COMIDs: \\n {comids[:20]} \\n\")\n",
    "\n",
    "if COMID in comids:\n",
    "    basin_idx = list(comids).index(COMID)\n",
    "else:\n",
    "    raise ValueError(f\"Catchment with ID {COMID} not found in the MERIT dataset.\")\n",
    "\n",
    "\n",
    "# 3. Get the data for the chosen catchment and plot.\n",
    "streamflow_pred_basin = pred[:, basin_idx].squeeze()\n",
    "\n",
    "plot_hydrograph(\n",
    "    timesteps,\n",
    "    streamflow_pred_basin,\n",
    "    streamflow_pred_basin,\n",
    "    resample=RESAMPLE,\n",
    "    title=f\"Hydrograph for Catchment {COMID}\",\n",
    "    ylabel='Streamflow (ft$^3$/s)',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrodl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
