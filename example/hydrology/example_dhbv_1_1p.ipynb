{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: **δHBV 1.1p**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates training and forward simulation with the δHBV 1.1p model developed by [Yalan Song et al. (2025)](https://doi.org/10.22541/essoar.172304428.82707157/v2). A pre-trained model is provided for those who only wish to run the model forward.\n",
    "\n",
    "For explanation of model structure, methodologies, data, and performance metrics, please refer to Song's publication [below](#publication). If you find this code is useful in your own work, please include the aforementioned citation.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Before Running:\n",
    "- **Environment**: See [setup.md](./../../docs/setup.md) for ENV setup. dMG must be installed with dependencies + hydroDL2 to run this notebook.\n",
    "\n",
    "- **Model**: Download pretrained δHBV 1.1p model weights from [AWS](https://mhpi-spatial.s3.us-east-2.amazonaws.com/mhpi-release/models/dHBV_1_1p_trained.zip). Then update the model config:\n",
    "\n",
    "    - In [`./generic_deltamodel/example/conf/config_dhbv_1_1p.yaml`](./../conf/config_dhbv_1_1p.yaml), update *trained_model* with your path to the parent directory containing both trained model weights `dHBV_1_1p_Ep50.pt` (or *Ep100*) **and** normalization file `normalization_statistics.json`.\n",
    "\n",
    "- **Data**: Download the CAMELS data extraction from [AWS](https://mhpi-spatial.s3.us-east-2.amazonaws.com/mhpi-release/camels/camels_data.zip). Then, updated the data configs:\n",
    "\n",
    "    - In [`./generic_deltamodel/example/conf/observations/camels_531.yaml`](./../conf/observations/camels_531.yaml) and [`camels_671.yaml`](./../conf/observations/camels_671.yaml), update...\n",
    "        1. *train_path* with `training_file` path,\n",
    "        2. *test_path* with `validation file` path,\n",
    "        3. *gage_info* with `gage_ids.npy` path,\n",
    "        4. *subset_path* with `531_subset.txt` path (camels_531 only).\n",
    "\n",
    "- **Hardware**: The NNs used in this model require CUDA support only available with Nvidia GPUs. For those without access, T4 GPUs can be used when running this notebook with dMG on [Google Colab](https://colab.research.google.com/).\n",
    "\n",
    "### Publication:\n",
    "\n",
    "*Yalan Song, Kamlesh Sawadekar, Jonathan M Frame, Ming Pan, Martyn Clark, Wouter J M Knoben, Andrew W Wood, Trupesh Patel, Chaopeng Shen. \"Physics-informed, Differentiable Hydrologic  Models for Capturing Unseen Extreme Events.\" ESS Open Archive (2025). https://doi.org/10.22541/essoar.172304428.82707157/v2.*\n",
    "\n",
    "<br>\n",
    "\n",
    "### Issues:\n",
    "For questions, concerns, bugs, etc., please reach out by posting an [issue](https://github.com/mhpi/generic_deltaModel/issues).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1.1 Train δHBV 1.1p\n",
    "\n",
    "After completing [these](#before-running) steps (model file not needed), train a δHBV 1.1p model with the code block below.\n",
    "\n",
    "**Note**\n",
    "- The settings defined in the config `./generic_deltamodel/example/conf/config_dhbv_1_1p.yaml` are set to replecate benchmark performance.\n",
    "- For model training, set `mode: train` in the config, or modify after config dict has been created (see below).\n",
    "- An `./example/generic_deltamodel/output/` directory will be generated to store experiment and model files. This location can be adjusted by changing the *save_path* key in your config. \n",
    "- If you are new to the dMG framework and want explanation of the methods used below, we suggest first looking at our [δHBV 1.0 tutorial](./../hydrology/example_dhbv_1_0.ipynb).\n",
    "- Default settings with 50 epochs, batch size of 100, and training window from 1 October 1999 to 30 September 2008 should use ~2.8GB of vram. Expect training times of ~8 hours with an Nvidia RTX 3090 Ti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# sys.path.append('../../')\n",
    "# sys.path.append('../../dMG')  # Add the dMG root directory.\n",
    "\n",
    "from core.utils import print_config\n",
    "from core.utils.factory import import_data_loader, import_trainer\n",
    "from example import load_config\n",
    "from models.model_handler import ModelHandler as dHBV\n",
    "\n",
    "#------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '../example/conf/config_dhbv_1_1p.yaml'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "config['mode'] = 'train'  # <-- Confirm that we are doing training if not set in the config file.\n",
    "print_config(config)\n",
    "\n",
    "# 2. Initialize the differentiable HBV 1.1p model (LSTM + HBV 1.1p).\n",
    "model = dHBV(config, verbose=True)\n",
    "\n",
    "# 3. Load and initialize a dataset dictionary of NN and HBV model inputs.\n",
    "data_loader_cls = import_data_loader(config['data_loader'])\n",
    "data_loader = data_loader_cls(config, test_split=True, overwrite=False)\n",
    "\n",
    "# 4. Initialize trainer to handle model training.\n",
    "trainer_cls = import_trainer(config['trainer'])\n",
    "trainer = trainer_cls(\n",
    "    config,\n",
    "    model,\n",
    "    train_dataset=data_loader.train_dataset,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 5. Start model training.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Evaluate Model Performance\n",
    "\n",
    "After completing the training in [1.1](#11-train--hbv-11p), or with the trained model provided, test $\\delta$ HBV 1.1p below on the evaluation data.\n",
    "\n",
    "--> For default settings expect evaluation time of ~5 minutes with an Nvidia RTX 3090.\n",
    "\n",
    "**Note**\n",
    "- For model evaluation, set `mode: test` in `example/conf/config_dhbv_1_1p.yaml`, or modify after the config dict has been created (see below).\n",
    "- When evaluating provided models, confirm that `test: test_epoch` in the config corresponds to your desired model (50 or 100 epochs).\n",
    "- The default evaluation window from 1 October 1989 to 30 September 1999 with `batch_size=25` should use ~2.7GB of vram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = torch.load('/projects/mhpi/leoglonz/project_silmaril/dMG/example/hydrology/output/camels_531/train1999-2008/no_multi/CudnnLstmModel_E100_R365_B100_H256_n16_Ln_prcp_WU_111111/HBV/RmseCombLoss/2dyn/parBETA_parBETAET/dHBV_Ep100.pt')\n",
    "dict['nn_model.linear_in.weight'] = dict.pop('nn_model.linearIn.weight')\n",
    "dict['nn_model.linear_in.bias'] = dict.pop('nn_model.linearIn.bias')\n",
    "dict['nn_model.linear_out.weight'] = dict.pop('nn_model.linearOut.weight')\n",
    "dict['nn_model.linear_out.bias'] = dict.pop('nn_model.linearOut.bias')\n",
    "\n",
    "torch.save(dict, '/projects/mhpi/leoglonz/project_silmaril/dMG/example/hydrology/output/camels_531/train1999-2008/no_multi/CudnnLstmModel_E100_R365_B100_H256_n16_Ln_prcp_WU_111111/HBV/RmseCombLoss/2dyn/parBETA_parBETAET/dHBV_Ep100.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../dMG')  # Add the dMG root directory.\n",
    "\n",
    "from core.utils import print_config\n",
    "from core.utils.factory import import_data_loader, import_trainer\n",
    "from example import load_config\n",
    "from models.model_handler import ModelHandler as dHBV\n",
    "\n",
    "#------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '../example/conf/config_dhbv_1_1p.yaml'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "config['mode'] = 'test'  # <-- Confirm that we are doing testing if not set in the config file.\n",
    "print_config(config)\n",
    "\n",
    "# 2. Initialize the differentiable HBV 1.1p model (LSTM + HBV 1.1p).\n",
    "model = dHBV(config, verbose=True)\n",
    "\n",
    "# 3. Load and initialize a dataset dictionary of NN and HBV model inputs.\n",
    "data_loader_cls = import_data_loader(config['data_loader'])\n",
    "data_loader = data_loader_cls(config, test_split=True, overwrite=False)\n",
    "\n",
    "# 4. Initialize trainer to handle model evaluation.\n",
    "trainer_cls = import_trainer(config['trainer'])\n",
    "trainer = trainer_cls(\n",
    "    config,\n",
    "    model,\n",
    "    eval_dataset=data_loader.eval_dataset,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 5. Start testing the model.\n",
    "print('Evaluating model...')\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Trained Model Performance\n",
    "\n",
    "After running evaluation on the model, a new directory (e.g., for a model trained for 50 epochs and tested from years 1989-1999), `test1989-1999_Ep50/` will be created in the same directory containing the model files. This path will be populated with...\n",
    "\n",
    "1. All model outputs (fluxes, states), including the target variable, *streamflow* (`flow_sim.npy`),\n",
    "\n",
    "2. `flow_sim_obs`, streamflow observation data for comparison against model predictions,\n",
    "\n",
    "2. `metrics.json`, containing evaluation metrics accross the test time range for every gage in the dataset,\n",
    "\n",
    "3. `metrics_agg.json`, containing evaluation metrics statistics across all gages (mean, median, standar deviation).\n",
    "\n",
    "4. `normalization_statistics.json`, containing statistics used for normalizing the testing data.\n",
    "\n",
    "\n",
    "We can use these outputs to visualize $\\delta$ HBV 1.1p's performance with a \n",
    "1. Cumulative distribution function (CDF) plot, \n",
    "\n",
    "2. CONUS map of gage locations and metric (e.g., NSE) performance.\n",
    "\n",
    "<br>\n",
    "\n",
    "But first, let's first check the (basin-)aggregated metrics for NSE, KGE, bias, RMSE, and, for both high/low flow regimes, RMSE and absolute percent bias..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from core.data import load_json\n",
    "from core.post import print_metrics\n",
    "\n",
    "print(f\"Evaluation output files saved to: {config['out_path']} \\n\")\n",
    "\n",
    "\n",
    "# 1. Load the basin-aggregated evaluation results.\n",
    "metrics_path = os.path.join(config['out_path'], 'metrics_agg.json')\n",
    "metrics = load_json(metrics_path)\n",
    "print(f\"Available metrics: {metrics.keys()} \\n\")\n",
    "\n",
    "# 2. Print the evaluation results.\n",
    "metric_names =  [\n",
    "    # Choose metrics to show.\n",
    "    'nse', 'kge', 'bias', 'rmse', 'rmse_low', 'rmse_high', 'flv_abs', 'fhv_abs',\n",
    "]\n",
    "print_metrics(metrics, metric_names, mode='median', precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the CDF Plot\n",
    "\n",
    "The cumulative distribution function (CDF) plot tells us what percentage (CDF on the y-axis) of basins performed at least better than a given metric on the evaluation data.\n",
    "We give an example of such a plot below for NSE, but you can adjust this to your preferred metric. See the output from the previous cell to see what metrics are available. (Note some may require changing `xbounds` in the `plot_cdf`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot CDF of the evaluation results.\n",
    "from core.post.plot_cdf import plot_cdf\n",
    "\n",
    "#------------------------------------------#\n",
    "# Choose the metric to plot. (See available metrics printed above, or in the metrics_agg.json file).\n",
    "METRIC = 'nse'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load the evaluation metrics.\n",
    "metrics_path = os.path.join(config['out_path'], 'metrics.json')\n",
    "metrics = load_json(metrics_path)\n",
    "\n",
    "# 2. Plot the CDF for NSE.\n",
    "plot_cdf(\n",
    "    metrics=[metrics],\n",
    "    metric_names=[METRIC],\n",
    "    model_labels=['dHBV 1.1p'],\n",
    "    title=r\"CDF of NSE for $\\delta$HBV 1.1p\",\n",
    "    xlabel=METRIC.capitalize(),\n",
    "    figsize=(8, 6),\n",
    "    xbounds=(0, 1),\n",
    "    ybounds=(0, 1),\n",
    "    show_arrow=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the Spatial Plot\n",
    "\n",
    "This plot shows the locations of each basin in the evaluation data, color-coded by performance on a metric. Here we give a plot for NSE, but as before, this metric can be changed to your preference. (See above for what is available; for metrics not valued between 0 and 1, you will need to set `dynamic_colorbar=True` in `geoplot_single_metric` to ensure proper coding.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the evaluation results spatially.\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from core.data import txt_to_array\n",
    "from core.post.plot_geo import geoplot_single_metric\n",
    "\n",
    "#------------------------------------------#\n",
    "# Choose the metric to plot. (See available metrics printed above, or in the metrics_agg.json file).\n",
    "METRIC = 'nse'\n",
    "\n",
    "# Set the paths to the gage id lists and shapefiles...\n",
    "GAGE_ID_PATH = 'your/path/to/gageid.npy'\n",
    "GAGE_ID_531_PATH = 'your/path/to/Sub531ID.txt'\n",
    "SHAPEFILE_PATH = 'your/path/to/camels_671_loc.shp'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# 2. Load gage ids + basin shapefile with geocoordinates (lat, long) for every gage.\n",
    "gage_ids = np.load(GAGE_ID_PATH, allow_pickle=True)\n",
    "gage_ids_531 = txt_to_array(GAGE_ID_531_PATH)\n",
    "coords = gpd.read_file(SHAPEFILE_PATH)\n",
    "\n",
    "# 3. Format geocoords for 531- and 671-basin CAMELS sets.\n",
    "coords_531 = coords[coords['gage_id'].isin(list(gage_ids_531))].copy()\n",
    "\n",
    "coords['gage_id'] = pd.Categorical(coords['gage_id'], categories=list(gage_ids), ordered=True)\n",
    "coords_531['gage_id'] = pd.Categorical(coords_531['gage_id'], categories=list(gage_ids_531), ordered=True)\n",
    "\n",
    "coords = coords.sort_values('gage_id')  # Sort to match order of metrics.\n",
    "basin_coords_531 = coords_531.sort_values('gage_id')\n",
    "\n",
    "# 4. Load the evaluation metrics.\n",
    "metrics_path = os.path.join(config['out_path'], 'metrics.json')\n",
    "metrics = load_json(metrics_path)\n",
    "\n",
    "# 5. Add the evaluation metrics to the basin shapefile.\n",
    "if config['observations']['name'] == 'camels_671':\n",
    "    coords[METRIC] = metrics[METRIC]\n",
    "    full_data = coords\n",
    "elif config['observations']['name'] == 'camels_531':\n",
    "    coords_531[METRIC] = metrics[METRIC]\n",
    "    full_data = coords_531\n",
    "else:\n",
    "    raise ValueError(f\"Observation data supported: 'camels_671' or 'camels_531'. Got: {config['observations']}\")\n",
    "\n",
    "# 6. Plot the evaluation results spatially.\n",
    "geoplot_single_metric(\n",
    "    full_data,\n",
    "    METRIC,\n",
    "    rf\"Spatial Map of {METRIC.upper()} for $\\delta$HBV 1.1p on CAMELS \" \\\n",
    "        f\"{config['observations']['name'].split('_')[-1]}\",\n",
    "    dynamic_colorbar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward $\\delta$ HBV 1.1p\n",
    "\n",
    "After completing [these](#before-running) steps, forward the $\\delta$ HBV 1.1p model with the code block below.\n",
    "\n",
    "Note:\n",
    "- The settings defined in `../example/conf/config_dhbv_1_1p.yaml` are set to replecate benchmark performance.\n",
    "- The default inference window is set from 1 October 2012 to 30 September 2014, which should use ~2.7GB of vram.\n",
    "- The first year (`warm_up` in the config, 365 days is default) of the inference period is used for initializing HBV's internal states (water storages) and is, therefore, excluded from the model's prediction output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../dMG')  # Add the dMG root directory.\n",
    "\n",
    "from core.utils import print_config\n",
    "from core.utils.factory import import_data_loader\n",
    "from example import load_config\n",
    "from models.model_handler import ModelHandler as dHBV\n",
    "\n",
    "#------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '../example/conf/config_dhbv_1_1p.yaml'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "config['mode'] = 'predict'  # <-- Confirm that we are doing training if not set in the config file.\n",
    "print_config(config)\n",
    "\n",
    "# 2. Initialize the differentiable HBV 1.1p model (LSTM + HBV 1.1p).\n",
    "model = dHBV(config, verbose=True)\n",
    "\n",
    "# 3. Load and initialize a dataset dictionary of NN and HBV model inputs.\n",
    "data_loader_cls = import_data_loader(config['data_loader'])\n",
    "data_loader = data_loader_cls(config, test_split=False, overwrite=False)\n",
    "\n",
    "# 4. Forward the model to get the predictions.\n",
    "predictions = model.forward(\n",
    "    data_loader.dataset,\n",
    "    eval=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Model Predictions\n",
    "\n",
    "After running model inference we can, e.g., view the hydrograph for one of the basins to see we are getting expected outputs.\n",
    "\n",
    "We can do this with our target variable, streamflow, for instance... (though, there are many other states and fluxes we can output as shown in the output cell below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from core.data import txt_to_array\n",
    "from core.post.plot_hydrograph import plot_hydrograph\n",
    "from core.utils.dates import Dates\n",
    "\n",
    "#------------------------------------------#\n",
    "# Choose a basin by USGS gage ID to plot.\n",
    "GAGE_ID = 1022500\n",
    "TARGET = 'flow_sim'\n",
    "\n",
    "# Resample to 3-day prediction. Options: 'D', 'W', 'M', 'Y'.\n",
    "RESAMPLE = '3D'\n",
    "\n",
    "# Set the paths to the gage ID lists...\n",
    "GAGE_ID_PATH = 'your/path/to/gageid.npy'\n",
    "GAGE_ID_531_PATH = 'your/path/to/Sub531ID.txt'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "print(f\"HBV states and fluxes: {predictions['HBV_1_1p'].keys()} \\n\")\n",
    "\n",
    "\n",
    "# 1. Get the streamflow predictions and daily timesteps of the prediction window.\n",
    "pred = predictions['HBV_1_1p'][TARGET]\n",
    "timesteps = Dates(config['predict'], config['dpl_model']['rho']).batch_daily_time_range\n",
    "\n",
    "# Remove warm-up period to match model output (see Note above.)\n",
    "timesteps = timesteps[config['dpl_model']['phy_model']['warm_up']:]\n",
    "\n",
    "\n",
    "# 2. Load the gage ID lists and get the basin index.\n",
    "gage_ids = np.load(GAGE_ID_PATH, allow_pickle=True)\n",
    "gage_ids_531 = txt_to_array(GAGE_ID_531_PATH)\n",
    "\n",
    "print(f\"First 20 available gage IDs: \\n {gage_ids[:20]} \\n\")\n",
    "print(f\"First 20 available gage IDs (531 subset): \\n {gage_ids_531[:20]} \\n\")\n",
    "\n",
    "if config['observations']['name'] == 'camels_671':\n",
    "    if GAGE_ID in gage_ids:\n",
    "        basin_idx = list(gage_ids).index(GAGE_ID)\n",
    "    else:\n",
    "        raise ValueError(f\"Basin with gage ID {GAGE_ID} not found in the CAMELS 671 dataset.\")\n",
    "\n",
    "elif config['observations']['name'] == 'camels_531':\n",
    "    if GAGE_ID in gage_ids_531:\n",
    "        basin_idx = list(gage_ids_531).index(GAGE_ID)\n",
    "    else:\n",
    "        raise ValueError(f\"Basin with gage ID {GAGE_ID} not found in the CAMELS 531 dataset.\")\n",
    "else:\n",
    "    raise ValueError(f\"Observation data supported: 'camels_671' or 'camels_531'. Got: {config['observations']}\")\n",
    "\n",
    "\n",
    "# 3. Get the data for the chosen basin and plot.\n",
    "streamflow_pred_basin = pred[:, basin_idx].squeeze()\n",
    "\n",
    "plot_hydrograph(\n",
    "    timesteps,\n",
    "    streamflow_pred_basin,\n",
    "    streamflow_pred_basin,\n",
    "    resample=RESAMPLE,\n",
    "    title=f\"Hydrograph for Gage ID {GAGE_ID}\",\n",
    "    ylabel='Streamflow (ft$^3$/s)',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrodl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
