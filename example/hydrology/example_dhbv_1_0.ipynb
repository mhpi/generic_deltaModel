{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Differentiable Modeling and **δHBV 1.0**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook gives detailed demonstration of training and forward simulation with the δHBV 1.0 model developed by [Dapeng Feng et al. (2022)](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2022WR032404). A pre-trained model is provided for those who only wish to run the model forward.\n",
    "\n",
    "For explanation of model structure, methodologies, data, and performance metrics, please refer to Feng's publication [below](#publication). If you find this code is useful in your own work, please include the aforementioned citation.\n",
    "\n",
    "*Note: We include some discussion of differentiable modeling methodology, and recommend this notebook as a starting point for an operational understanding of the dMG framework*\n",
    "\n",
    "<br>\n",
    "\n",
    "### Before Running:\n",
    "- **Environment**: See [setup.md](./../../docs/setup.md) for ENV setup. dMG must be installed with dependencies + hydroDL2 to run this notebook.\n",
    "\n",
    "- **Model**: Download pretrained δHBV 1.0 model weights from [AWS](https://mhpi-spatial.s3.us-east-2.amazonaws.com/mhpi-release/models/dHBV_1_0_trained.zip). Then update the model config:\n",
    "\n",
    "    - In [`./generic_deltamodel/example/conf/config_dhbv_1_0.yaml`](./../conf/config_dhbv_1_0.yaml), update *trained_model* with your path to the parent directory containing both trained model weights `dHBV_1_0_Ep50.pt` (or *Ep100*) **and** normalization file `normalization_statistics.json`.\n",
    "    - **Note**: make sure this path includes the last closing forward slash: e.g., `./your/path/to/model/`.\n",
    "\n",
    "- **Data**: Download the CAMELS data extraction from [AWS](https://mhpi-spatial.s3.us-east-2.amazonaws.com/mhpi-release/camels/camels_data.zip). Then, updated the data configs:\n",
    "\n",
    "    - In [`./generic_deltamodel/example/conf/observations/camels_531.yaml`](./../conf/observations/camels_531.yaml) and [`camels_671.yaml`](./../conf/observations/camels_671.yaml), update...\n",
    "        1. *data_path* with `camels_dataset` path,\n",
    "        2. *gage_info* with `gage_ids.npy` path,\n",
    "        3. *subset_path* with `531_subset.txt` path (camels_531 only).\n",
    "\n",
    "    - The full 671-basin or 531-basin CAMELS datasets can be selected by setting `observations: camels_671` or `camels_531` in the model config, respectively.\n",
    "\n",
    "- **Hardware**: The NNs used in this model require CUDA support only available with Nvidia GPUs. For those without access, T4 GPUs can be used when running this notebook with dMG on [Google Colab](https://colab.research.google.com/).\n",
    "\n",
    "<br>\n",
    "\n",
    "### Publication:\n",
    "\n",
    "*Dapeng Feng, Jiangtao Liu, Kathryn Lawson, Chaopeng Shen. \"Differentiable, Learnable, Regionalized Process‐Based Models With Multiphysical Outputs can Approach State‐Of‐The‐Art Hydrologic Prediction Accuracy.\" Water Resources Research 58, no. 10 (2022): e2022WR032404. https://doi.org/10.1029/2022WR032404.*\n",
    "\n",
    "<br>\n",
    "\n",
    "### Issues:\n",
    "For questions, concerns, bugs, etc., please reach out by posting an [issue](https://github.com/mhpi/generic_deltaModel/issues).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Differentiable Modeling\n",
    "\n",
    "In general, differentiable modeling represents the coupling of a neural network and a (differentiable) physical model. This enables several capabilities, like introducing missing processses and bias corrections. In the applications of these notebooks, we demonstrate the parameter learning modality;\n",
    "\n",
    "Physics models include parameters for which True values are seldom known, but can be approximated with various methods. By coupling a neural network (NN), we can learn a set of a physical model's parameters (static or dynamic in, e.g., time), which can then be passed alongside other input variables to the physical model for making predictions.\n",
    "\n",
    "For δHBV 1.0, we use an LSTM NN in concert with the physical model HBV (Beck 2020; Seibert 2005). HBV uses input forcing (time-varying) variables precipitation, temperature, and potential evapotranspiration (PET) across a collection of hydrologic basins, with physical parameters learned at the same spatiotemporal resolution [timesteps, basins], to make predictions about hydrologic states and fluxes (e.g., streamflow in our case) in both space and time. In general, this differentiable model takes the form\n",
    "\n",
    "$\n",
    "\\delta \\text{HBV} = \n",
    "\\begin{cases}\n",
    "P = \\text{LSTM}(X \\text{, \\ } A) \\\\\n",
    "Q \\text{, \\ } \\mu = \\text{HBV}(X \\text{, \\ } P)\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "where\n",
    "- \\( $P$ \\) the physical parameters used in HBV's equations, learned by a NN;\n",
    "- \\( $X$ \\) represents input weather forcings;\n",
    "- \\( $A$ \\) is the set of static basin attributes. (This could be any other static data correlated with $P$);\n",
    "- \\( $Q, \\mu$ \\) these are the fluxes and states, respectively, that the physical model can output.\n",
    "\n",
    "Currently, δHBV is trained to make **streamflow** predictions, but it can be reconfigured without much effort to predict percolation, recharge, and groundwater flow, among others.\n",
    "\n",
    "After showing an example implementation, we will demonstrate how to train the model and expose critical details of the process.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Forward δHBV 1.0\n",
    "\n",
    "After completing [these](#before-running) steps, δHBV 1.0 can be built with the code cells [below](#13-demonstration), where we illustrate the model creation process in detail.\n",
    "\n",
    "See [Section 2](#2-forward-δhbv-10-short-version) for a high-level demonstration.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1.1 Background\n",
    "\n",
    "To create δHBV 1.0 in dMG, we interface with the repository [HydroDL2](https://github.com/mhpi/hydroDL2) containing hydrologic models including those used in these tutorials ([setup.md](./../../docs/setup.md) details this connection). \n",
    "\n",
    "<br>\n",
    "\n",
    "- #### Set Model, Experiment, Dataset Configurations\n",
    "\n",
    "    Two flexible configuration files exist for augmenting behaviors of the dMG framework: One defines model settings and training/testing parameters, and another defines parameters for your dataset (observations). For this tutorial, two such configuration files have been created and require minimal preparation to use in this notebook:\n",
    "    \n",
    "    1. [`./generic_deltamodel/example/conf/config_dhbv_1_0.yaml`](./../conf/config_dhbv_1_0.yaml) -- Model/experiment settings\n",
    "    2. [`./generic_deltamodel/example/conf/observations/camels_531.yaml`](./../conf/observations/camels_531.yaml) and [`camels_671.yaml`](./../conf/observations/camels_671.yaml) -- CAMELS 531- and 671-basin datasets.\n",
    "\n",
    "    With these, all aspects of dMG model creation, training, testing, etc. can be controlled. As is, the model/experiment config is setup to reproduce benchmark results for [δHBV 1.0](#publication) (see [here](https://mhpi.github.io/benchmarks/#10-year-training-comparison)) using the CAMELS 531-basin subset of weather forcings and static basin attributes. Full 671-basin benchmark models can also be trained/tested, and both can be configured by setting the following options in the model config:\n",
    "\n",
    "    - For CAMELS 531-basin, 10-year benchmark (Default):\n",
    "        - `observations: camels_531`\n",
    "        - `train:` 1999/10/01 to 2008/09/30 (`start_time` to `end_time`)\n",
    "        - `test:` 1989/10/01 to 1999/09/30\n",
    "\n",
    "    - For CAMELS 671-basin, 15-year benchmark:\n",
    "        - `observations: camels_671`\n",
    "        - `train:` 1980/10/01 to 1995/09/30 (`start_time` to `end_time`)\n",
    "        - `test:` 1995/10/01 to 2010/09/30\n",
    "\n",
    "    See [configuration_files.md](./../../docs/configuration_files.md) for more details.\n",
    "\n",
    "<br>\n",
    "\n",
    "- #### Building a Model \n",
    "\n",
    "    There are two ways to build a differentiable model in dMG which leverage its framework-package duality:\n",
    "    - **Implicit (framework)**: Best for small-scale experiments and distribution of final products.\n",
    "\n",
    "        Add/change modules in dMG to create your own differentiable model, and tailor model and dataset configuration files to reflect desired model and experiment behaviors. (Modules like trainers, physics models, neural networks, loss functions, and data loaders/samplers are designed to be hot-swappable per user needs.)\n",
    "\n",
    "        With these this done, dMG can be run with\n",
    "\n",
    "        ```bash\n",
    "        cd ./generic_deltamodel\n",
    "        python src/dMG/__main__.py\n",
    "        ```\n",
    "\n",
    "    - **Explicit (Package)**: Best for exploratory research and prototyping (illustrated in the code block below).\n",
    "\n",
    "        This approach is similar in that we still use config files to handle settings (though a manually-generated dictionary object could also be used). The difference is that we are able to expose the fundamental steps in the modeling process by using dMG like a package; data preprocesing, model building, and experimentation/forwarding can be inported from dMG. In doing so, we make it\n",
    "        quicker to develop model and data pipelines, and easier to follow internal processes without framework-level abstraction.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1.2 Walkthrough\n",
    "\n",
    "The following is an explicit implementation of dMG to create and forward δHBV 1.0:\n",
    "\n",
    "1.  **Load a configuration file**: Using Hydra and OmegaConf packages, we can convert the model/dataset configs into a dictionary object `config`. For example, if your config file contains `mode: train`, the dictionary yields `config['mode'] == 'train'`. However, the config can also contain sub-dictionaries. For instance, \n",
    "    \n",
    "    ```yaml\n",
    "    training: \n",
    "        start_time: 1999/10/01\n",
    "    ```\n",
    "\n",
    "    which is accessed like `config['training']['start_time'] == '1999/10/01'`.\n",
    "\n",
    "    Once the config is loaded, we use the function *set_randomseed* to fix randomseeds for NumPy and PyTorch for reproducibility. This is given by the key `random_seed` in the config.\n",
    "\n",
    "2.  **Initialize sub-models**: Next, we initialize the NN and physics model our differentiable model will use, in this case an LSTM from `dMG/models/neural_networks/cudnn_lstm.py` and HBV 1.0 from [HydroDL2](https://github.com/mhpi/hydroDL2).\n",
    "\n",
    "3.  **Load in data**: At this step, we load and process our data as a dictionary of variable and attribute datasets that are used by the NN and physics model. This dataset_dict is created by a data_loader, and should meet minimum requirements of the base class `dMG/core/data/loaders/base.py`.\n",
    "\n",
    "    For this example, we take a small, arbitrarily selected sample of the data to illustrate the modeling process.\n",
    "\n",
    "4.  **Create a differentiable model**: Now, the sub-models are linked together by a differentiable model wrapper (`DeltaModel`). This has the effect of interfacing both models to achieve the desired modality, e.g., having the LSTM generate parameters for HBV. \n",
    "\n",
    "5.  **Forward/Experiment**: With the differentiable model created, it can be forwarded (as demonstrated\n",
    "    below), or applied in any user-defined experiments.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1.3 Demonstration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from hydroDL2.models.hbv.hbv import HBV\n",
    "\n",
    "from dMG.core.data.loaders import HydroLoader\n",
    "from dMG.core.utils import load_nn_model, print_config, set_randomseed\n",
    "from dMG.models.delta_models import DplModel\n",
    "from example import load_config, take_data_sample\n",
    "\n",
    "#------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '../example/conf/config_dhbv_1_0.yaml'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "config['mode'] = 'simulation'  # <-- Confirm that we are doing forward.\n",
    "print_config(config)\n",
    "\n",
    "# Set random seed for reproducibility.\n",
    "set_randomseed(config['random_seed'])\n",
    "\n",
    "# 2. Initialize physical model and NN.\n",
    "device = config['device']\n",
    "phy_model = HBV(config['delta_model']['phy_model'])\n",
    "nn = load_nn_model(phy_model, config['delta_model'])\n",
    "\n",
    "# 3. Load and initialize a dataset dictionary of NN and HBV model inputs.\n",
    "# Take a sample to reduce size on GPU.\n",
    "dataset_dict = HydroLoader(config).dataset\n",
    "dataset_sample = take_data_sample(config, dataset_dict, days=730, basins=100)\n",
    "\n",
    "# 4. Create the differentiable model dHBV 1.0: a torch.nn.Module describing how\n",
    "# the NN is linked to the physical model HBV.\n",
    "dpl_model = DplModel(phy_model=phy_model, nn_model=nn).to(device)\n",
    "\n",
    "## From here, forward or train dpl_model just as any torch.nn.Module model.\n",
    "\n",
    "# 5. For example, to forward:\n",
    "output = dpl_model.forward(dataset_sample)\n",
    "\n",
    "\n",
    "print(\"-------------\\n\")\n",
    "print(f\"Streamflow predictions for {output['streamflow'].shape[0]} days and \" \\\n",
    "      f\"{output['streamflow'].shape[1]} basins ~ \\nShowing the first 5 days for \" \\\n",
    "        f\"first basin: \\n {output['streamflow'][:5,:1].cpu().detach().numpy().squeeze()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Forward δHBV 1.0 (Short version)\n",
    "\n",
    "This is an abbreviation of the forward demonstration in [1.3](#13-demonstration). Be sure to complete [these](#before-running) steps before running.\n",
    "\n",
    "Note:\n",
    "- The settings defined in the config `./generic_deltamodel/example/conf/config_dhbv_1_0p.yaml` are set to replicate benchmark performance.\n",
    "- The first year (`warm_up` in the config, default is 365 days) of the inference period is used for initializing HBV's internal states (water storages) and is, therefore, excluded from the model's prediction output.\n",
    "\n",
    "### 2.1 Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from dMG import ModelHandler\n",
    "from dMG.core.utils import import_data_loader, print_config, set_randomseed\n",
    "from example import load_config\n",
    "\n",
    "#------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '../example/conf/config_dhbv_1_0.yaml'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "config['mode'] = 'simulation'\n",
    "print_config(config)\n",
    "\n",
    "# Set random seed for reproducibility.\n",
    "set_randomseed(config['random_seed'])\n",
    "\n",
    "# 2. Initialize the differentiable HBV 1.0 model (LSTM + HBV 1.0).\n",
    "model = ModelHandler(config, verbose=True)\n",
    "\n",
    "# 3. Load and initialize a dataset dictionary of NN and HBV model inputs.\n",
    "data_loader_cls = import_data_loader(config['data_loader'])\n",
    "data_loader = data_loader_cls(config, test_split=False, overwrite=False)\n",
    "\n",
    "# 4. Forward the model to get the predictions.\n",
    "output = model.forward(\n",
    "    data_loader.dataset,\n",
    "    eval=True,\n",
    ")\n",
    "\n",
    "print(\"-------------\\n\")\n",
    "print(f\"Streamflow predictions for {output['HBV']['streamflow'].shape[0]} days and \" \\\n",
    "      f\"{output['HBV']['streamflow'].shape[1]} basins ~ \\nShowing the first 5 days for \" \\\n",
    "        f\"first basin: \\n {output['HBV']['streamflow'][:5,:1].cpu().detach().numpy().squeeze()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualizing Model Predictions\n",
    "\n",
    "After running model inference we can, e.g., view the hydrograph for one of the basins to see we are getting expected outputs.\n",
    "\n",
    "We can do this with our target variable, streamflow, for instance (though, there are many other states and fluxes we can view -- see cell output below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from dMG.core.data import txt_to_array\n",
    "from dMG.core.post import plot_hydrograph\n",
    "from dMG.core.utils import Dates\n",
    "\n",
    "#------------------------------------------#\n",
    "# Choose a basin by USGS gage ID to plot.\n",
    "GAGE_ID = 1022500\n",
    "TARGET = 'streamflow'\n",
    "\n",
    "# Resample to 3-day prediction. Options: 'D', 'W', 'M', 'Y'.\n",
    "RESAMPLE = '3D'\n",
    "\n",
    "# Set the paths to the gage ID lists...\n",
    "GAGE_ID_PATH = config['observations']['gage_info']  #./gage_id.npy\n",
    "GAGE_ID_531_PATH = config['observations']['subset_path']  #./531sub_id.txt\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "# 1. Get the streamflow predictions and daily timesteps of the prediction window.\n",
    "print(f\"HBV states and fluxes: {list(output['HBV'].keys())} \\n\")\n",
    "\n",
    "pred = output['HBV'][TARGET]\n",
    "timesteps = Dates(config['simulation'], config['delta_model']['rho']).batch_daily_time_range\n",
    "\n",
    "# Remove warm-up period to match model output (see Note above.)\n",
    "timesteps = timesteps[config['delta_model']['phy_model']['warm_up']:]\n",
    "\n",
    "\n",
    "# 2. Load the gage ID lists and get the basin index.\n",
    "gage_ids = np.load(GAGE_ID_PATH, allow_pickle=True)\n",
    "gage_ids_531 = txt_to_array(GAGE_ID_531_PATH)\n",
    "\n",
    "print(f\"First 20 available gage IDs: \\n {gage_ids[:20]} \\n\")\n",
    "print(f\"First 20 available gage IDs (531 subset): \\n {gage_ids_531[:20]} \\n\")\n",
    "\n",
    "if config['observations']['name'] == 'camels_671':\n",
    "    if GAGE_ID in gage_ids:\n",
    "        basin_idx = list(gage_ids).index(GAGE_ID)\n",
    "    else:\n",
    "        raise ValueError(f\"Basin with gage ID {GAGE_ID} not found in the CAMELS 671 dataset.\")\n",
    "\n",
    "elif config['observations']['name'] == 'camels_531':\n",
    "    if GAGE_ID in gage_ids_531:\n",
    "        basin_idx = list(gage_ids_531).index(GAGE_ID)\n",
    "    else:\n",
    "        raise ValueError(f\"Basin with gage ID {GAGE_ID} not found in the CAMELS 531 dataset.\")\n",
    "else:\n",
    "    raise ValueError(f\"Observation data supported: 'camels_671' or 'camels_531'. Got: {config['observations']}\")\n",
    "\n",
    "\n",
    "# 3. Get the data for the chosen basin and plot.\n",
    "streamflow_pred_basin = pred[:, basin_idx].squeeze()\n",
    "\n",
    "plot_hydrograph(\n",
    "    timesteps,\n",
    "    streamflow_pred_basin,\n",
    "    resample=RESAMPLE,\n",
    "    title=f\"Hydrograph for Gage ID {GAGE_ID}\",\n",
    "    ylabel='Streamflow (mm/day)',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Train δHBV 1.0\n",
    "\n",
    "Now that we can build the model, we train it and expose critical steps in the process below.\n",
    "\n",
    "See [Section 3](#31-training--hbv-10----abbreviated) for abbreviated training code.\n",
    "\n",
    "### 3.1 Load Config and Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from dMG.core.data.loaders import HydroLoader\n",
    "from example import load_config\n",
    "\n",
    "#------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '../example/conf/config_dhbv_1_0.yaml'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "# Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "config['mode'] = 'train'  # <-- Confirm that we are doing training.\n",
    "\n",
    "# Get training dataset\n",
    "train_dataset = HydroLoader(config, test_split=True).train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Initialize Model and Optimizer\n",
    "\n",
    "These are the auxillary tasks completed by the trainer before beginning the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from hydroDL2.models.hbv.hbv import HBV\n",
    "\n",
    "from dMG.core.utils import load_criterion, load_nn_model, set_randomseed\n",
    "from dMG.models.delta_models import DplModel\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility.\n",
    "set_randomseed(config['random_seed'])\n",
    "\n",
    "# Initialize physical model and neural network\n",
    "phy_model = HBV(config['delta_model']['phy_model'])\n",
    "nn = load_nn_model(phy_model, config['delta_model'])\n",
    "\n",
    "# Create the differentiable model dHBV:\n",
    "device = config['device']\n",
    "dpl_model = DplModel(phy_model=phy_model, nn_model=nn).to(device)\n",
    "print(f\"δHBV model: \\n ----- \\n {dpl_model}\")\n",
    "\n",
    "# Init an Adadelta optimizer\n",
    "optimizer = torch.optim.Adadelta(\n",
    "    dpl_model.parameters(),\n",
    "    lr=config['delta_model']['nn_model']['learning_rate'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training\n",
    "\n",
    "A basic training loop is given for training the LSTM in δHBV 1.0 to optimize HBV's parameters for streamflow predictions.\n",
    "\n",
    "#### Key Steps in the Training Loop\n",
    "1. **Create Training Grid**  \n",
    "   The `create_training_grid` function calculates the training settings:\n",
    "   - `n_samples`: The number of samples in the dataset.\n",
    "   - `n_minibatch`: The number of samples to process per epoch.\n",
    "   - `n_timesteps`: The number of timesteps per sample.\n",
    "\n",
    "2. **Epoch Loop**  \n",
    "   Each epoch represents one full cycle through the training data. `total_loss` is reset to track the total error across all batches within each epoch.\n",
    "\n",
    "3. **Batch Loop**  \n",
    "   Within each epoch, the code processes data in smaller chunks (minibatches) to improve training efficiency and avoid oversaturation of GPU VRAM. Each minibatch is randomly sampled with replacement from the training set, with the *n_minibatch* being a sufficient batch size to guarantee full dataset coverage during each epoch.\n",
    "   \n",
    "   For each batch:\n",
    "   - **Sample Data**: `HydroDataSampler` randomly selects a sample of training data for the batch.\n",
    "   - **Forward Pass**: The model processes the input data to produce predictions.\n",
    "   - **Calculate Loss**: `loss_func` compares predictions to observed values to calculate the error for the batch.\n",
    "   - **Backward Pass and Optimization**: \n",
    "     - `loss.backward()` computes gradients to adjust the LSTM's weights.\n",
    "     - `optimizer.step()` updates the LSTM weights.\n",
    "     - `optimizer.zero_grad()` resets gradients for the next batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from dMG.core.data import create_training_grid\n",
    "from dMG.core.data.samplers import HydroSampler\n",
    "from dMG.core.utils import save_model\n",
    "\n",
    "\n",
    "# init a loss function\n",
    "loss_func = load_criterion(train_dataset['target'], config['loss_function'], device=device)\n",
    "\n",
    "# Initialize training sampler.\n",
    "sampler = HydroSampler(config)\n",
    "\n",
    "# Get target variable for training.\n",
    "target = config['train']['target'][0]\n",
    "\n",
    "# Number of training samples per epoch, batch size, and number of timesteps.\n",
    "n_samples, n_minibatch, n_timesteps = create_training_grid(\n",
    "    train_dataset['xc_nn_norm'],\n",
    "    config,\n",
    ")\n",
    "\n",
    "# Start of training.\n",
    "for epoch in range(1, config['train']['epochs'] + 1):\n",
    "    total_loss = 0.0  # Initialize epoch loss to zero.\n",
    "\n",
    "    prog_str = f\"Epoch {epoch}/{config['train']['epochs']}\"\n",
    "\n",
    "    # Work through training data in batches.\n",
    "    for _ in tqdm(range(1, n_minibatch + 1), desc=prog_str,\n",
    "                       leave=False, dynamic_ncols=True):\n",
    "\n",
    "        # Take a sample of the training data for the batch.\n",
    "        dataset_sample = sampler.get_training_sample(\n",
    "            train_dataset,\n",
    "            n_samples,\n",
    "            n_timesteps,\n",
    "        )\n",
    "\n",
    "        # Forward pass through dPL model.\n",
    "        predictions = dpl_model.forward(dataset_sample)\n",
    "\n",
    "        # Calculate loss.\n",
    "        loss = loss_func(\n",
    "            predictions[target],\n",
    "            dataset_sample['target'],\n",
    "            n_samples=dataset_sample['batch_sample'],\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / n_minibatch + 1\n",
    "    print(f\"Avg model loss after epoch {epoch}: {avg_loss}\")\n",
    "\n",
    "    # Save the model every save_epoch (set in the config).\n",
    "    model_name = config['delta_model']['phy_model']['model']\n",
    "    if epoch % config['train']['save_epoch'] == 0:\n",
    "        save_model(config, dpl_model, model_name, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Train δHBV 1.0 (Short Version)\n",
    "\n",
    "The high level dMG training loop for δHBV 1.0 is given in the proceeding.\n",
    "\n",
    "**Note**\n",
    "- The settings defined in the config `./generic_deltamodel/example/conf/config_dhbv_1_0p.yaml` are set to replicate benchmark performance on 531 CAMELS basins.\n",
    "- For model training, set `mode: train` in the config, or modify after config dict has been created (see below).\n",
    "- An `./example/generic_deltamodel/output/` directory will be generated to store experiment and model files. This location can be adjusted by changing the *save_path* key in your config. \n",
    "- Default settings with 50 epochs, batch size of 100, and training window from 1 October 1999 to 30 September 2008 should use ~2.8GB of VRAM. Expect training times of ~8 hours with an Nvidia RTX 3090 Ti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from dMG import ModelHandler\n",
    "from dMG.core.utils import (import_data_loader, import_trainer, print_config,\n",
    "                            set_randomseed)\n",
    "from example import load_config\n",
    "\n",
    "#------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '../example/conf/config_dhbv_1_0.yaml'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "config['mode'] = 'train'\n",
    "print_config(config)\n",
    "\n",
    "# Set random seed for reproducibility.\n",
    "set_randomseed(config['random_seed'])\n",
    "\n",
    "# 2. Initialize the differentiable HBV 1.0 model (LSTM + HBV 1.0) with model handler.\n",
    "model = ModelHandler(config, verbose=True)\n",
    "\n",
    "# 3. Load and initialize a dataset dictionary of NN and HBV model inputs.\n",
    "data_loader_cls = import_data_loader(config['data_loader'])\n",
    "data_loader = data_loader_cls(config, test_split=True, overwrite=False)\n",
    "\n",
    "\n",
    "# 4. Initialize trainer to handle model training.\n",
    "trainer_cls = import_trainer(config['trainer'])\n",
    "trainer = trainer_cls(\n",
    "    config,\n",
    "    model,\n",
    "    train_dataset=data_loader.train_dataset,\n",
    ")\n",
    "\n",
    "# 5. Start model training.\n",
    "trainer.train()\n",
    "print(f'Training complete. Model saved to \\n{config['model_path']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 5. Evaluate Model Performance\n",
    "\n",
    "After completing the training in [Section 3](#3-train-δhbv-10), or with the trained model provided, test δHBV 1.0 below on the evaluation data.\n",
    "\n",
    "If using the pretrained models, either\n",
    "- Enter the path to the directory containing these models in the config, or\n",
    "- Add the models to the `./example/hydrology/output/.../parBETA_parBETAET/` directory generated by the previous code blocks. If this does not yet exist, run the block below to generate and add the model files.\n",
    "\n",
    "**Note**\n",
    "- For model evaluation, set `mode: test` in the config, or modify after config dict has been created (see below).\n",
    "- When evaluating provided models, confirm that `test.test_epoch` in the config corresponds the training epochs completed for the model you want to test (e.g., 50 or 100).\n",
    "- Default settings with 50 epochs, batch size of 25, and testing window from 1 October 1989 to 30 September 1999 should use ~2.7GB of VRAM. Expect evalutation times of ~5 minutes with an Nvidia RTX 3090 Ti.\n",
    "\n",
    "### 5.1 Streamflow Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from dMG import ModelHandler\n",
    "from dMG.core.utils import import_data_loader, import_trainer, print_config\n",
    "from example import load_config\n",
    "\n",
    "#------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '../example/conf/config_dhbv_1_0.yaml'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "# 1. Load configuration dictionary of model parameters and options.\n",
    "config = load_config(CONFIG_PATH)\n",
    "config['mode'] = 'test'\n",
    "print_config(config)\n",
    "\n",
    "set_randomseed(config['random_seed'])\n",
    "\n",
    "# 2. Initialize the differentiable HBV 1.0 model (LSTM + HBV 1.0).\n",
    "model = ModelHandler(config, verbose=True)\n",
    "\n",
    "# 3. Load and initialize a dataset dictionary of NN and HBV model inputs.\n",
    "data_loader_cls = import_data_loader(config['data_loader'])\n",
    "data_loader = data_loader_cls(config, test_split=True, overwrite=False)\n",
    "\n",
    "# 4. Initialize trainer to handle model evaluation.\n",
    "trainer_cls = import_trainer(config['trainer'])\n",
    "trainer = trainer_cls(\n",
    "    config,\n",
    "    model,\n",
    "    eval_dataset=data_loader.eval_dataset,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 5. Start testing the model.\n",
    "print('Evaluating model...')\n",
    "trainer.evaluate()\n",
    "print(f'Metrics and predictions saved to \\n{config['out_path']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualize Trained Model Performance\n",
    "\n",
    "Once the model has been evaluated, a new directory (e.g., for a model trained for 50 epochs and tested from years 1989-1999), `test1989-1999_Ep50/`, will be created in the same directory containing the model files. This path will be populated with...\n",
    "\n",
    "1. All model outputs (fluxes, states), including the target variable, *streamflow* (`streamflow.npy`),\n",
    "\n",
    "2. `streamflow_obs.npy`, streamflow observation data for comparison against model predictions,\n",
    "\n",
    "2. `metrics.json`, containing evaluation metrics accross the test time range for every gage in the dataset,\n",
    "\n",
    "3. `metrics_agg.json`, containing evaluation metric statistics across all sites (mean, median, standard deviation).\n",
    "\n",
    "We can use these outputs to visualize δHBV 1.0's performance with a \n",
    "1. Cumulative distribution function (CDF) plot, \n",
    "\n",
    "2. CONUS map of gage locations and metric (e.g., NSE) performance.\n",
    "\n",
    "<br>\n",
    "\n",
    "But first, let's first check the (basin-)aggregated metrics for NSE, KGE, bias, RMSE, and, for both high/low flow regimes, RMSE and absolute percent bias..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dMG.core.data import load_json\n",
    "from dMG.core.post import print_metrics\n",
    "\n",
    "\n",
    "print(f\"Evaluation output files saved to: {config['out_path']} \\n\")\n",
    "\n",
    "# 1. Load the basin-aggregated evaluation results.\n",
    "metrics_path = os.path.join(config['out_path'], 'metrics_agg.json')\n",
    "metrics = load_json(metrics_path)\n",
    "print(f\"Available metrics: {metrics.keys()} \\n\")\n",
    "\n",
    "# 2. Print the evaluation results.\n",
    "metric_names =  [\n",
    "    # Choose metrics to show.\n",
    "    'nse', 'kge', 'bias', 'rmse', 'rmse_low', 'rmse_high', 'flv_abs', 'fhv_abs',\n",
    "]\n",
    "print_metrics(metrics, metric_names, mode='median', precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 CDF Plot\n",
    "\n",
    "The cumulative distribution function (CDF) plot tells us what percentage (CDF on the y-axis) of basins performed at least better than a given metric on the evaluation data.\n",
    "\n",
    "An example is given below for NSE, but you can change to your preferred metric (see the output from the previous cell), but note some may require changing *xbounds* in *plot_cdf()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dMG.core.post import plot_cdf\n",
    "\n",
    "#------------------------------------------#\n",
    "# Choose the metric to plot. (See available metrics printed above, or in the metrics_agg.json file).\n",
    "METRIC = 'nse'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "# 1. Load the evaluation metrics.\n",
    "metrics_path = os.path.join(config['out_path'], 'metrics.json')\n",
    "metrics = load_json(metrics_path)\n",
    "\n",
    "# 2. Plot the CDF for NSE.\n",
    "plot_cdf(\n",
    "    metrics=[metrics],\n",
    "    metric_names=[METRIC],\n",
    "    model_labels=['dHBV 1.0'],\n",
    "    title=\"CDF of NSE for δHBV 1.0\",\n",
    "    xlabel=METRIC.capitalize(),\n",
    "    figsize=(8, 6),\n",
    "    xbounds=(0, 1),\n",
    "    ybounds=(0, 1),\n",
    "    show_arrow=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Spatial Plot\n",
    "\n",
    "This plot shows the locations of each basin in the evaluation data, color-coded by performance on a metric. Here we give a plot for NSE, but as before, this can be changed to your preference. (See above; for metrics not valued between 0 and 1, you will need to set `dynamic_colorbar=True` in `geoplot_single_metric` to ensure proper coding.)\n",
    "\n",
    "Note, you will need to add paths to the CAMELS shapefile, gage IDs, and 531-gage subset which can be found in the [CAMELS download](#before-running)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dMG.core.data import txt_to_array\n",
    "from dMG.core.post import geoplot_single_metric\n",
    "\n",
    "#------------------------------------------#\n",
    "# Choose the metric to plot. (See available metrics printed above, or in the metrics_agg.json file).\n",
    "METRIC = 'nse'\n",
    "\n",
    "# Set the paths to the gage id lists and shapefiles...\n",
    "GAGE_ID_PATH = config['observations']['gage_info']  #./gage_id.npy\n",
    "GAGE_ID_531_PATH = config['observations']['subset_path']  #./531sub_id.txt\n",
    "SHAPEFILE_PATH = './your/path/to/camels_loc/camels_671_loc.shp'\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "# 1. Load gage ids + basin shapefile with geocoordinates (lat, long) for every gage.\n",
    "gage_ids = np.load(GAGE_ID_PATH, allow_pickle=True)\n",
    "gage_ids_531 = txt_to_array(GAGE_ID_531_PATH)\n",
    "coords = gpd.read_file(SHAPEFILE_PATH)\n",
    "\n",
    "# 2. Format geocoords for 531- and 671-basin CAMELS sets.\n",
    "coords_531 = coords[coords['gage_id'].isin(list(gage_ids_531))].copy()\n",
    "\n",
    "coords['gage_id'] = pd.Categorical(coords['gage_id'], categories=list(gage_ids), ordered=True)\n",
    "coords_531['gage_id'] = pd.Categorical(coords_531['gage_id'], categories=list(gage_ids_531), ordered=True)\n",
    "\n",
    "coords = coords.sort_values('gage_id')  # Sort to match order of metrics.\n",
    "basin_coords_531 = coords_531.sort_values('gage_id')\n",
    "\n",
    "# 3. Load the evaluation metrics.\n",
    "metrics_path = os.path.join(config['out_path'], 'metrics.json')\n",
    "metrics = load_json(metrics_path)\n",
    "\n",
    "# 4. Add the evaluation metrics to the basin shapefile.\n",
    "if config['observations']['name'] == 'camels_671':\n",
    "    coords[METRIC] = metrics[METRIC]\n",
    "    full_data = coords\n",
    "elif config['observations']['name'] == 'camels_531':\n",
    "    coords_531[METRIC] = metrics[METRIC]\n",
    "    full_data = coords_531\n",
    "else:\n",
    "    raise ValueError(f\"Observation data supported: 'camels_671' or 'camels_531'. Got: {config['observations']}\")\n",
    "\n",
    "# 5. Plot the evaluation results spatially.\n",
    "geoplot_single_metric(\n",
    "    full_data,\n",
    "    METRIC,\n",
    "    rf\"Spatial Map of {METRIC.upper()} for δHBV 1.0 on CAMELS \" \\\n",
    "        f\"{config['observations']['name'].split('_')[-1]}\",\n",
    "    dynamic_colorbar=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
